{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\n",
    "\n",
    "**Outline of the \"story\" of the note**:\n",
    "1. Given a generative model $G$, there is generally three things that we might want to do.  \n",
    "    a. Sampling  \n",
    "    b. Estimation  \n",
    "    c. ~~Likelihood evaluation~~\n",
    "2. Remember what the the original GAN objective  \n",
    "3. Introduce the f-divergence family  \n",
    "    a. General definintion  \n",
    "    b. Maybe show, e.g., KL divergence is in that family  \n",
    "4. Variational estimation of $f$-divergences\n",
    "5. Leads to new objective  \n",
    "    a. General form  \n",
    "    b. Compare to the objective of original GAN  \n",
    "    c. Need to comply to supports of $f$ (activation function on $G$)  \n",
    "6. Algorithm for optimizing new objective\n",
    "    a. Algorithm\n",
    "    b. Theorem for convergence\n",
    "7. Showtime 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[[1]](http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.pdf) Nowozin, S., Cseke, B. and Tomioka, R., 2016. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems (pp. 271-279).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
