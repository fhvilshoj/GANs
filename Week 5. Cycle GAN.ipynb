{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code example for talk, not meant to be read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt # plot how normal distribution changes. \n",
    "\n",
    "\"\"\"\n",
    "    Construct a very simple generator and a somehow larger discriminator. \n",
    "    Train the generator on N(0,1) with real data N(4,2) and plot each iteration. \n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "def plot(): \n",
    "    ax.cla()\n",
    "    ax.hist(generator.predict_on_batch(noise) , alpha=0.3, label=\"Fake p_g\")\n",
    "    ax.hist(real_data , alpha=0.3, label=\"Real p_d\")\n",
    "    xs = np.arange(-8, 8, 0.1)\n",
    "    pred = discriminator.predict_on_batch(xs)\n",
    "    ax.plot(xs, pred*250, label=\"D(x)\")\n",
    "    ax.legend(loc=3)\n",
    "    ax.set_ylim([0, 280])\n",
    "    ax.set_xlim([-9, 9])\n",
    "    \n",
    "    fig.suptitle(\"Iteration: [%i / %i]\"%(i, iterations))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.canvas.draw()\n",
    "    plt.pause(.01)\n",
    "\n",
    "# number of samples. \n",
    "n = 1000\n",
    "iterations = 100000\n",
    "repeat = 10\n",
    "\n",
    "# define generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(1, input_dim=1)) # one neuron except bias, don't have the relu activation!\n",
    "\n",
    "# define discrimiantor\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(10, input_dim=1, activation=\"relu\")) # non linearity has some use here. \n",
    "discriminator.add(Dense(1,  activation=\"sigmoid\"))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Combine models\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "discriminator.trainable = False # from the gan model we freeze discriminator to use it as loss function\n",
    "gan.add(discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Do several updates, plot some interval of iterations fake / real data. \n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    noise     = np.random.normal(0, 1, size=(n, 1))\n",
    "    fake_data = generator.predict_on_batch(noise)\n",
    "    real_data = np.random.normal(4, 2, size=(n, 1))\n",
    "    \n",
    "    disc_X = np.concatenate((real_data, fake_data), axis=0)\n",
    "    disc_y = np.concatenate((np.zeros(n), np.ones(n)), axis=0) # flip labels since we min instead of max. \n",
    "    \n",
    "    plot()\n",
    "    for j in range(repeat): discriminator.train_on_batch(x=disc_X, y=disc_y)\n",
    "    for j in range(repeat): gan.train_on_batch(x=noise, y=np.zeros(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate between normal distributions, e.g. \n",
    "\n",
    "$p_x = N(\\mu_x, \\sigma^2_x)$ and $p_y = N(\\mu_y, \\sigma^2_y)$ and we don't know the specific values of $\\mu_x,\\mu_x$ and $\\sigma_x, \\sigma_y$\n",
    "\n",
    "extend afterwards to more interesting case, e.g. translate between cifar dogs and cats. \n",
    "\n",
    "\n",
    "Inspired by https://github.com/eriklindernoren/Keras-GAN/blob/master/cyclegan/cyclegan.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Input\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt # plot how normal distribution changes. \n",
    "\n",
    "\"\"\"\n",
    "    Translate between different normal distributions. \n",
    "    p_x = N(2, 1) and p_y = N(-2, 1)\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3)) \n",
    "\n",
    "def plot(display): \n",
    "    ax[0].cla()\n",
    "    ax[0].hist(X, alpha=0.3, label=\"X\")\n",
    "    ax[0].hist(Y, alpha=0.3, label=\"Y\")\n",
    "    \n",
    "    xs = np.arange(-5, 15, 0.1)\n",
    "    \n",
    "    if display == 1 or display == 2: \n",
    "        ax[0].hist(fake_X, alpha=0.3, label=\"Fake X\")\n",
    "        ax[0].hist(fake_Y, alpha=0.3, label=\"Fake Y\") \n",
    "        \n",
    "        ax[0].plot(xs, DX.predict(xs)*250, label=\"DX\")\n",
    "        ax[0].plot(xs, DY.predict(xs)*250, label=\"DY\")\n",
    "    \n",
    "    if display == 0 or display == 2:\n",
    "        ax[0].hist(rec_X, alpha=0.3, label=\"Rec X\")\n",
    "        ax[0].hist(rec_Y, alpha=0.3, label=\"Rec Y\") \n",
    "    \n",
    "    ax[0].legend(loc=1)\n",
    "    ax[0].set_ylim([0, 280])\n",
    "    ax[0].set_xlim([-7, 12])\n",
    "    \n",
    "    fig.suptitle(\"Iteration: [%i / %i]\"%(i, iterations))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.canvas.draw()\n",
    "    plt.savefig(\"gifs/cycle_gan_normals/%i.png\"%i)\n",
    "    plt.pause(.01)\n",
    "\n",
    "# number of samples. \n",
    "n          = 1000\n",
    "iterations = 1000\n",
    "repeat     = 10\n",
    "\n",
    "# For both F and G we know a single neuron with bias is sufficient. \n",
    "\n",
    "\n",
    "# G: X -> Y\n",
    "G = Sequential()\n",
    "G.add(Dense(1, input_dim=1))\n",
    "\n",
    "# DY: discriminate {y} and {G(x)}\n",
    "DY = Sequential()\n",
    "DY.add(Dense(10, input_dim=1, activation=\"relu\")) \n",
    "DY.add(Dense(1,  activation=\"sigmoid\"))\n",
    "DY.compile(loss='binary_crossentropy', optimizer=Adam(0.005), metrics=['acc'])\n",
    "\n",
    "# F: Y -> X\n",
    "F = Sequential()\n",
    "F.add(Dense(1, input_dim=1))\n",
    "\n",
    "# DX: discriminate {x} and {F(y)}\n",
    "DX = Sequential()\n",
    "DX.add(Dense(10, input_dim=1, activation=\"relu\")) \n",
    "DX.add(Dense(1,  activation=\"sigmoid\"))\n",
    "DX.compile(loss='binary_crossentropy', optimizer=Adam(0.005), metrics=['acc'])\n",
    "\n",
    "input_X = Input(shape=(1, ))\n",
    "input_Y = Input(shape=(1, ))\n",
    "\n",
    "fake_X = F(input_Y)\n",
    "fake_Y = G(input_X)\n",
    "\n",
    "rec_X = F(fake_Y)\n",
    "rec_Y = G(fake_X)\n",
    "\n",
    "DX.trainable = False \n",
    "DY.trainable = False # freeze discriminator to use it as loss function\n",
    "\n",
    "adv_X = DX(fake_X)\n",
    "adv_Y = DY(fake_Y)\n",
    "\n",
    "combined = Model(inputs =[input_X, input_Y], \n",
    "                 outputs=[adv_X,   adv_Y,\n",
    "                          rec_X,   rec_Y])\n",
    "\n",
    "combined.compile(loss         = ['binary_crossentropy', 'binary_crossentropy', 'mae', 'mae'],\n",
    "                 loss_weights = [1, 1, 5, 5],\n",
    "                 optimizer    = Adam(0.01) )\n",
    "\n",
    "# Adversarial loss ground truths\n",
    "valid = np.ones(n)\n",
    "fake  = np.zeros(n)\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(\"\\r--- [%i / %i] ---\"%(i, iterations), end=\"\")\n",
    "    \n",
    "    X = np.random.normal(-4, 1, size=(n, 1))\n",
    "    Y = np.random.normal(+4, 1, size=(n, 1))\n",
    "    \n",
    "    fake_X = F.predict(Y)\n",
    "    fake_Y = G.predict(X)\n",
    "    \n",
    "    rec_X = F.predict(fake_Y)\n",
    "    rec_Y = G.predict(fake_X)\n",
    "\n",
    "    # Update discriminator\n",
    "    for j in range(10):\n",
    "        DY.train_on_batch(Y, valid)\n",
    "        DY.train_on_batch(fake_Y, fake) \n",
    "\n",
    "        DX.train_on_batch(X, valid)\n",
    "        DX.train_on_batch(fake_X, fake)\n",
    "    \n",
    "    # Update generator\n",
    "    combined.train_on_batch([X, Y], [valid, valid, X, Y])\n",
    "    \n",
    "    if i % 10 == 0: \n",
    "        plot(1)\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
