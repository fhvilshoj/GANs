{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "In this section we introduce an unsupervised learning problem, argue why this problem seems completely unreasonable to solve, and then show how Generative Adversarial Networks (GANs) provide an ingenious approach to solving the problem. By the end of the notebook you will be able to use GANs to generate do the following: \n",
    "\n",
    "<img src=\"gifs/n(0,1) to n(4,2) simple/gif.gif\">\n",
    "\n",
    "\n",
    "<b>Problem</b>: Suppose you have samples $x_1,...,x_n\\sim p_d$ and you want to sample from $p_d$, that is, produce samples $\\hat{x}\\sim p_d$. \n",
    "\n",
    "Let us first consider a simple special case of this problem: \n",
    "<br><br>\n",
    "\n",
    "<div style=\"width: 80%; border: 1px solid black; padding: 12px; margin-left: 10%; background-color: #efefef;\">\n",
    "    <b>Example:</b> <br>\n",
    "    The above general problem seems very hard. But suppose we actually knew that $p_d=N(\\mu, \\sigma^2)$. We could then estimate the sample mean $\\hat{\\mu}$ and sample standard deviation $\\hat{\\sigma}$:\n",
    "    <br><br>\n",
    "    <div style=\"text-align: center; width:100%;\">$\\hat{\\mu}=\\frac{1}{n} \\sum_{i=1}^N x_i\\quad\\quad\\quad \\hat{\\sigma}=\\sqrt{\\frac{\\sum_{i=1}^N (x_i-\\hat{\\mu})^2}{(N-1}}$</div>\n",
    "    <br>\n",
    "    We can then sample from  $x\\sim N(\\hat{\\mu}, \\hat{\\sigma}^2)$. This approach can actually be shown to yield the <a href=\"https://en.wikipedia.org/wiki/Maximum_likelihood_estimation\">MLE</a>. \n",
    "</div>\n",
    "\n",
    "Unfortunately, the data we are typically interested in does <b>not</b> follow a nice distributed in any way. Imagine a distribution of dog images, human speech or natural language. These distributions are remarkably complicated, assuming that they follow a nice distribution like the normal distribution is completely unreasonable. \n",
    "\n",
    "That said, we might be able to approxmiate the data distribution $p_d$ by transforming a normal distribution using a very complicated function $g$. In other words, consider the implicit distribution $p_g$ that arrises by transforming $z\\sim N(0, 1)$ using a function $g$ so $g(z)=p_g$. If we find a $g$ such that $p_g$ approximates $p_d$ we could sample $z\\sim N(0,1)$ and then compute $g(z)$. \n",
    "\n",
    "To formalize \"$p_g$ <i>approximates</i> $p_d$\" we define an error function between $p_g$ and $p_d$ and attempt to solve the optimization problem that arises: \n",
    "\n",
    "$$g^*=\\text{arg }\\text{min}_{g\\in G} \\text{ error}(p_g, p_d)\\quad\\quad\\text{for hypothesis set } G$$\n",
    "\n",
    "This is similar to how we usually approach supervised machine learning. In supervised learning, we want to approximate some unkown target function $f:\\mathcal{X}\\rightarrow \\mathcal{Y}$. This is done by solving the optimization problem that arises by defining an error function which can then be minimized. This is the exact same approach we applied above. \n",
    "\n",
    "There exists a group of functions called <i>divergences</i> which provide a measure on how different one distribution is from another. One example is the Kullback-Leibler divergence (KL divergence for short):\n",
    "\n",
    "$$D_{KL}(p_g, p_d) = \\sum_x p_d(x) \\log \\left( \\frac{ p_d(x) } { p_g(x)} \\right) $$\n",
    "\n",
    "By using the KL divergence as error function we can formulate our problem as: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g^*&=\\text{arg }\\text{min}_{g\\in G} \\; D_{KL}(p_g, p_d)\\\\\n",
    "&=\\text{arg }\\text{min}_{g\\in G} \\sum_x p_d(x) \\log \\left( \\frac{ p_d(x) } { p_g(x)} \\right)\\quad\\quad(1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unfortunately, attempting to solve this problem seems completely out of reach, simply computing $D_{KL}$ is troublesome:\n",
    "1. There is no obvious way for us to compute $p_d(x)$ or $p_g(x)$. Furthermore, our goal is to generate samples from $p_d(x)$, if we knew a way to efficiently compute $p_d(x)$ the problem is essentially solved. \n",
    "2. The sum over $x$ is a sum over all images. Even if we have small $32$ times $32$ grayscale images where each pixel is one byte, there would be $255^{32^2}$ images.\n",
    "\n",
    "It turns out that different GAN variants can be shown to minimize different variants of (1) without explicitly compute any divergences. This was shown in a followup article [1] we cover in Week 2. In the following section we introduce GANs and provide some insight into the theoretical results of the first article. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Generative Adversarial Networks\n",
    "Our goal is to find a function $g$ that implicitly defines a probability distribution $p_g$ by transforming $p_z=N(0,1)$ into fakes $g(z)=p_g$, such that fake samples $f_i\\sim p_g$ are indistinguishable from real samples $x_i\\sim p_d$. Instead of using a divergence to measure error between $p_g$ and $p_d$, we introduce a function $d$ which attempts to distinguish between real and fake sampels. We usually call $g$ the generator and $d$ the discriminator. See the illustration below: \n",
    "\n",
    "<br><div class=\"figure\" style=\"width: 100%; text-align: center; \">\n",
    "<img src=\"figs/week1_gan.png\" style=\"width: 80%;\" ><br>\n",
    "    <b>Figure 1: </b>A schematic of a Generative Adversarial Network.  \n",
    "</div>\n",
    "\n",
    "The discriminator recieves both real images $x_1,...,x_n\\sim p_d$ and fake images from the generator $g(z_1),...,g(z_n)\\sim p_g$. The objective of the discriminator is to predict $1$ on real images $x$ and $0$ on fake images $g(z)$. On the other hand, the objective of the generator is to fool the discriminator into predicting $1$ on a fake image $g(z)$. The objectives of the generator and discriminator are thus conflicting, the generator wins if the discriminator losses (see <a href=\"https://en.wikipedia.org/wiki/Zero-sum_game\">zero-sum games</a>). \n",
    "\n",
    "The game is captured by the following equation:\n",
    "\n",
    "$$\\text{min }_g\\text{ max}_d\\; \\mathbb{E}_{x\\sim p_d}[\\text{log d(x)}] + \\mathbb{E}_{z \\sim p_z}[1-\\text{log(d(g(z))}] \\quad\\quad(2)$$\n",
    "\n",
    "<b>Discriminator: </b><br>\n",
    "Consider (2) without the $\\text{min}_g$ by fixed some $g$. The first term with $d(x)$ for $x\\sim p_D$ corresponds to maximizing the discriminators probability estimate of real data being real. The second term $1-d(g(z))$ for $z\\sim p_z$ (which is maximized by $d(g(z))=0$) corresponds to maximizing its probability estimate of fake data being fake. In practice we approximate the expectations by using a training sample $D_{disc}$ for which we minimize cross entropy: \n",
    "\n",
    "$$ D_{disc}=\\{(x_1, 1), \\cdots , (x_n, 1), (g(z_1), 0), \\cdots , (g(z_n), 0) \\}$$\n",
    "\n",
    "$$\\text{max}_d \\sum_{(x,y)\\in D_{disc}} y\\text{ log }d(x) + (1-y)\\text{ log }(1-d(x)) $$\n",
    "\n",
    "Notice that the problem from the perspective of the discriminator is completely supervised, even though the labels are artificial in the sense they were not made but human, but are a product of the way the discrimiantor and generator relate to each other. \n",
    "\n",
    "<b>Generator: </b><br>\n",
    "The first term of (2) does not depend on the generator. The second term $1-d(g(z))$ for $z\\sim p_z$ corresponds to minimizing the discriminators probability estimate of fake data being fake; in other words, fool the discriminator. Again, in practice we approximate the expectation by using a training sample $D_{gen}$: \n",
    "\n",
    "$$ D_{gen}=\\{ (x_1, 1), ..., (x_n, 1), (z_1, 0), ..., (z_n, 0) \\} $$\n",
    "\n",
    "$$\\text{min}_g \\sum_{(x,y)\\in D_{gen}} y\\text{ log }d(x) + (1-y)\\text{ log }(1-d(g(x))) $$\n",
    "\n",
    "Since we are minimizing with respect to $g$ we can equivalently minimize the equation below without the first term: \n",
    "\n",
    "$$\\text{min}_g \\sum_{(x,y)\\in D_{gen}} (1-y)\\text{ log }(1-d(g(x))) $$\n",
    "\n",
    "$$ D_{gen}=\\{ (z_1, 0), ..., (z_n, 0) \\} $$\n",
    "\n",
    "\n",
    "In this sense the discriminator $d$ plays a vital role in the error function of the generator $g$. Notice that the problem from the perspective of the generator is completely supervised! \n",
    "\n",
    "<b>Both smaller problems are supervised!</b><br>\n",
    "Notice that the two problems from the perspective of both $g$ and $d$ are supervised. In this sen, the GANs split the unsupervised learning problem into two supervised learning problems by introducing the discriminator $d$ and propising adversarial training. \n",
    "\n",
    "That said, the supervised learning problems are moving. From the perspective of $d$ the data changes as $g$ becomes better at generating fake images. From the perspetive of $g$ the labels change as $d$ becomes better to distinguish between fake and real images. This causes training to be unstable. \n",
    "\n",
    "It turns out [1] that iteratively training $g$ and $d$ approximately minimizes (2). In that sense the point remains, GANs allow us to approximately minimize our original unsupervised learning problem for which we could not even evaluate how good we are performing, byt splitting the problem into to supervised subproblems. The details of this is postponed to next week, we briefly comment that the loss in (2) does not correspond to approximately minimizing the KL divergence, but that it indeed correspond to approximately minimizing some divergence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theorem 1 of [2]\n",
    "\n",
    "\n",
    "><b>Proposition 1.</b> For $g$ fixed, the optimal discriminator $d$ is\n",
    ">$$d^*_g(x)=\\frac{p_{data}(x)}{p_{data}{x}+p_g(x)}$$\n",
    "\n",
    "explain intuition, and give a sketch of proof. \n",
    "\n",
    "> <b>Theorem 1.</b> The global minimum of the virtual training criterion C(G) is achieved if and only if\n",
    "> p g = p data . At that point, C(G) achieves the value − log 4. (see [2] Theorem 1)\n",
    "\n",
    "explain intuition and give a sketch of proof. \n",
    "\n",
    "<!-- If we remove the $\\text{min]_g$ from equation (2) and write it as a function $C(G)$ we get the loss for a specific $g$ against the best adversary $d$. Intuitively, if $g$ recovers the true data distribution so $p_g=p_d$ the discriminator is faced with a tough problem; it recieves real data and fake data that is indistinguishable from real data. Since the fake data is indistinguishable from real data, the discriminator is forced to flip a coin and predict 50/50 percent on all data. The theorem states that this is the best option for $g$. -->\n",
    "\n",
    "\n",
    "> <b>Proposition 2.</b> If G and D have enough capacity, and at each step of Algorithm 1, the discriminator\n",
    ">is allowed to reach its optimum given G, and p g is updated so as to improve the criterion\n",
    "\n",
    ">$$\\mathbb{E}_{x\\sim p_{data}}[\\text{log }d^*_g(x)]+\\mathbb{E}_{x\\sim p_g}[\\text{log }(1-d^*_g(x))]$$\n",
    "\n",
    ">then p g converges to p data\n",
    "\n",
    "explain intuition and give a sketch of proof. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The following section provides three examples of training GANs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "## Transforming $N(0, 1)$ to $N(4, 2)$.\n",
    "In this section we use a gan to translate $N(0, 1)$ to $N(4, 2)$. Explain network architecture, how the single weight and bias corresponds to transforming the normal distribution $N(b, w)$.  \n",
    "\n",
    "<img src=\"gifs/n(0,1) to n(4,2)/gif.gif\">\n",
    "\n",
    "\n",
    "Explain the abstract idea of how the code implements the generator and the discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt # plot how normal distribution changes. \n",
    "\n",
    "\"\"\"\n",
    "    Construct a very simple generator and a somehow larger discriminator. \n",
    "    Train the generator on N(0,1) with real data N(4,2) and plot each iteration. \n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "def plot(): \n",
    "    ax.cla()\n",
    "    ax.hist(generator.predict_on_batch(noise) , alpha=0.3, label=\"Fake p_g\")\n",
    "    ax.hist(real_data , alpha=0.3, label=\"Real p_d\")\n",
    "    xs = np.arange(-8, 8, 0.1)\n",
    "    pred = discriminator.predict_on_batch(xs)\n",
    "    ax.plot(xs, pred*250, label=\"D(x)\")\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 280])\n",
    "    ax.set_xlim([-9, 9])\n",
    "    \n",
    "    fig.suptitle(\"Iteration: [%i / %i]\"%(i, iterations))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.canvas.draw()\n",
    "    plt.savefig(\"gifs/n(0,1) to n(4,2)/%i.png\"%i)\n",
    "    plt.pause(.01)\n",
    "\n",
    "# number of samples. \n",
    "n = 1000\n",
    "iterations = 200\n",
    "repeat = 10\n",
    "\n",
    "# define generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(1, input_dim=1)) # one neuron except bias, don't have the relu activation!\n",
    "generator.add(Dense(1))\n",
    "\n",
    "# define discrimiantor\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(10, input_dim=1, activation=\"relu\")) # non linearity has some use here. \n",
    "discriminator.add(Dense(1,  activation=\"sigmoid\"))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Combine models\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "discriminator.trainable = False # from the gan model we freeze discriminator to use it as loss function\n",
    "gan.add(discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Do several updates, plot some interval of iterations fake / real data. \n",
    "\n",
    "for i in tqdm(range(iterations)):\n",
    "    \n",
    "    noise     = np.random.normal(0, 1, size=(n, 1))\n",
    "    fake_data = generator.predict_on_batch(noise)\n",
    "    real_data = np.random.normal(4, 2, size=(n, 1))\n",
    "    \n",
    "    disc_X = np.concatenate((real_data, fake_data), axis=0)\n",
    "    disc_y = np.concatenate((np.zeros(n), np.ones(n)), axis=0) # flip labels since we min instead of max. \n",
    "    \n",
    "    plot()\n",
    "    for j in range(repeat): discriminator.train_on_batch(x=disc_X, y=disc_y)\n",
    "    for j in range(repeat): gan.train_on_batch(x=noise, y=np.zeros(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming $U(0,1)^{64}$ to MNIST\n",
    "\n",
    "\n",
    "TODO: find an architecture that works somewhere, and reference that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt # plot how normal distribution changes. \n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\"\"\"\n",
    "    Same as above but on MNIST with arhictecture inspired by [github?/X] \n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(5, 2))\n",
    "\n",
    "def plot(fake_data, current_iteration): \n",
    "    fig.suptitle(\"Iteration: [%i / %i]\"%(current_iteration, iterations))\n",
    "\n",
    "    for i in range(4): \n",
    "        ax[i].imshow(fake_data[0].reshape(28, 28), cmap=\"gray\")\n",
    "        ax[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.canvas.draw()\n",
    "    plt.savefig(\"gifs/mnist/%i.png\"%current_iteration)\n",
    "    plt.pause(.01)\n",
    "\n",
    "# number of samples. \n",
    "iterations = 2000\n",
    "repeat = 10\n",
    "mini_batch = 32\n",
    "n = 60000\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255 # normalize to [0,1]\n",
    "x_train = x_train.reshape(60000, 28**2)\n",
    "\n",
    "# define generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128, input_dim=64, activation=\"relu\")) \n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Dense(256, activation=\"relu\"))  \n",
    "generator.add(BatchNormalization())\n",
    "generator.add(Dense(28**2, activation=\"sigmoid\"))\n",
    "\n",
    "# define discrimiantor\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(512, input_dim=28**2, activation=\"relu\")) \n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(Dense(256, activation=\"relu\")) \n",
    "discriminator.add(BatchNormalization())\n",
    "discriminator.add(Dense(1,  activation=\"sigmoid\"))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Combine models\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "discriminator.trainable = False # from the gan model we freeze discriminator to use it as loss function\n",
    "gan.add(discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "\n",
    "# Do several updates, plot some interval of iterations fake / real data. \n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    print(\"\\r[%i / %i]\"%(i, iterations), end=\"\")\n",
    "    noise     = np.random.uniform(0, 1, size=(mini_batch, 64))\n",
    "    fake_data = generator.predict_on_batch(noise)\n",
    "    real_data = x_train[np.random.permutation(n)[:mini_batch]]\n",
    "              \n",
    "    disc_X = np.concatenate((real_data, fake_data), axis=0)\n",
    "    disc_y = np.concatenate((np.zeros(mini_batch), np.ones(mini_batch)), axis=0) # flip labels since we min instead of max. \n",
    "    \n",
    "    if i % 200 == 0: plot(fake_data, i)\n",
    "    for j in range(repeat): discriminator.train_on_batch(x=disc_X, y=disc_y)\n",
    "    for j in range(repeat): gan.train_on_batch(x=noise, y=np.zeros(mini_batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] https://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization\n",
    "\n",
    "[2] (https://scholar.google.com/scholar_url?url=http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&hl=en&sa=T&oi=gsb-gga&ct=res&cd=1&d=11977070277539609369&ei=bMBKXLjGDJKemQHOwZm4Dg&scisig=AAGBfm1_PAGbFd1N3ZcQl_vwVHCEeKz1FQ) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
