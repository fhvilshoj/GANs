{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\n",
    "\n",
    "In the [last note](Week%201.%20Generative%20Adversarial%20Networks.ipynb), we saw how to learn a distribution $p_g$ that approximated some unknown distribution $p_d$. We did this by introducing two neural networks. Namely, the generator and the discriminator, which were both neural networks. The discriminator and the generator played a game where the discriminator were to distinguish between real data sampled from $p_d$ and fake data sampled from the generator. The generator, on the other hand, were to fool the discriminator by transforming random noise $z_i \\sim \\mathcal{N}(0, 1)$ into something indistinguishable from the real data.\n",
    "\n",
    "In essence, fooling the discriminator corresponds to minimizing the \"distance\" between $p_d$ and $p_g$ for some notion of distance. We presented the [Kullbach-Liebler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) as one such measure and argued that we were not able to calculate this measure for several reasons. The divergence is defined as\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\int_\\mathcal{X} p(x) \\log \\frac{p(x)}{q(x)} dx \\quad \\quad (1)\n",
    "$$\n",
    "\n",
    "over the domain $\\mathcal{X}$ for continuous probability density functions $p$ and $q$. As we could not calculate this directly, we introduced the discriminator in order to be able to opgimize the generator. More formally, the discriminato played the following game:\n",
    "\n",
    "$$\n",
    "\\text{min }_g\\text{ max}_d\\; \\mathbb{E}_{x\\sim p_d}[\\text{log d(x)}] + \\mathbb{E}_{z \\sim p_z}[1-\\text{log(d(g(z))}] \\quad \\quad (2)\n",
    "$$\n",
    "\n",
    "When playing this game, [[3]](#References) showed that under certain constraints, the optimal\n",
    "discriminator $d^\\ast_g(x)$ is the following\n",
    "\n",
    "$$\n",
    "d^\\ast_g(x) = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}. \\quad \\quad (3)\n",
    "$$\n",
    "\n",
    "Further more, it turned out that playing this game would approximately minimize the [Jensen-Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)\n",
    "\n",
    "$$\n",
    "D_{JS}(P || Q) = D_{KL}(P ||P + Q) + D_{KL}(Q || P + Q). \\quad \\quad (4)\n",
    "$$\n",
    "\n",
    "The goal of this note is to generalize Eqn. (4) to a broader class of divergences. Conceptually, this will allow us to \"change the cost function\" (which is essentially Eqn. (2)). It will also generalize Eqn. (3) such that the optimal discriminator will get different forms according to which divergence the GAN setup is minimizing.\n",
    "\n",
    "A simple motivation for why we may want to change the objective of the training is the gif in Figure 1, which is very similar to the one we saw in the previous note. The only difference is that the real data now comes from a multi-modal distribution. The figure shows how the learned distribution (which is now underspecified since it can only represent \"simple\" uni-modal distribution) may learn to simply span the two modes of the data. It may be that we would rather want the generater to just focus on one of the modes of the true distribution. In that case we could hope that a change in traning objective would favour such a property.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"gifs/dual-mode/gif.gif\">\n",
    "    <strong>Figure 1</strong> Animation of how $p_g$ converges towards $p_d$.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing GANs to f-divergences\n",
    "\n",
    "As stated above, the goal of this note is to generalize GANs to minimize a broader class of divergences. Specifically, we will show that we can generalize GANs to minimize any [f-divergence](https://en.wikipedia.org/wiki/F-divergence). In order to do so, we need three ingredients. \n",
    "\n",
    "1. Not surprisingly, the first one is the f-divergence. \n",
    "2. The second ingredient is the [convex conjugate](https://en.wikipedia.org/wiki/Convex_conjugate) of convex lower semi-continuous functions.\n",
    "3. [Jensens Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) is the third and final ingredient.\n",
    "\n",
    "The structure of the following sections is as follows. First, we describe what the three ingredients are and how we will use them. Second, we will use the ingredients to show a lower bound on the f-divergence, which is very close to the GAN objective in (2). Finally, we will discuss how to use this lower bound to minimize different f-divergences.  \n",
    "\n",
    "### f-divergences\n",
    "Until now, we have seen two divergences. First, we saw the KL-divergence (Eqn. (1)) and second we saw the JS-divergence (Eqn. (4)). It turns out that these divergences are both part of a broughter class of divergences called the f-divergences. f-divergences are characterized by a convex and continuous function $f$ where $f(1) = 0$. The divergence is defined as follows\n",
    "\n",
    "$$\n",
    "D_f(P||Q) = \\int q(x) f\\left( \\frac{p(x)}{q(x)} \\right) \\quad \\quad (5)\n",
    "$$\n",
    "\n",
    "Let's veryfi the the KL-divergence is actually a part of this family. Let $f(x) = u \\log u$ and pug it into Eqn. (5):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_f(P || Q) &= \\int q(x) \\left( \\frac{p(x)}{q(x)} \\log \\frac{p(x)}{q(x)} \\right)\\\\\n",
    "&= \\int p(x) \\log \\frac{p(x)}{q(x)} \\\\\n",
    "&= D_{KL}(P || Q)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly, if we let $f(x) = u \\log u + (u + 1) \\log (u + 1)$, we recover the JS-divergence (up to a constant).\n",
    "\n",
    "### The Convex Conjugate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "n       = 1000\n",
    "x1      = np.random.normal(0, 1, size=(n, 1))\n",
    "x2      = np.random.normal(4, 1, size=(n, 1))\n",
    "x       = np.c_[x1, x2].reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "def plot(): \n",
    "    ax.cla()\n",
    "    ax.hist(generator.predict_on_batch(noise) , alpha=0.3, label=\"Fake p_g\")\n",
    "    ax.hist(real_data , alpha=0.3, label=\"Real p_d\")\n",
    "    xs = np.arange(-8, 16, 0.1)\n",
    "    pred = discriminator.predict_on_batch(xs)\n",
    "    ax.plot(xs, pred*250, label=\"D(x)\")\n",
    "    ax.legend(loc=3)\n",
    "    ax.set_ylim([0, 280])\n",
    "    ax.set_xlim([-9, 13])\n",
    "    \n",
    "    fig.suptitle(\"Iteration: [%i / %i]\"%(i, iterations))\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.canvas.draw()\n",
    "    plt.savefig(\"gifs/dual-mode/%i.png\"%i)\n",
    "    plt.pause(.01)\n",
    "\n",
    "# number of samples. \n",
    "n = 1000\n",
    "iterations = 200\n",
    "repeat = 10\n",
    "\n",
    "# define generator\n",
    "generator = Sequential()\n",
    "generator.add(Dense(1, input_dim=1)) # one neuron except bias, don't have the relu activation!\n",
    "#generator.add(Dense(1))\n",
    "\n",
    "# define discrimiantor\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(10, input_dim=1, activation=\"relu\")) # non linearity has some use here. \n",
    "discriminator.add(Dense(1,  activation=\"sigmoid\"))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.005))\n",
    "\n",
    "# Combine models\n",
    "gan = Sequential()\n",
    "gan.add(generator)\n",
    "discriminator.trainable = False # from the gan model we freeze discriminator to use it as loss function\n",
    "gan.add(discriminator)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.005))\n",
    "\n",
    "# Do several updates, plot some interval of iterations fake / real data. \n",
    "for i in range(iterations):\n",
    "    noise     = np.random.normal(0, 1, size=(n, 1))\n",
    "    fake_data = generator.predict_on_batch(noise)\n",
    "    \n",
    "    x1        = np.random.normal(4, 1, size=(n//2, 1))\n",
    "    x2        = np.random.normal(8, 1, size=(n//2, 1))\n",
    "    real_data = np.c_[x1, x2].reshape(-1, 1)\n",
    "    \n",
    "    disc_X = np.concatenate((real_data, fake_data), axis=0)\n",
    "    disc_y = np.concatenate((np.zeros(n), np.ones(n)), axis=0) # flip labels since we min instead of max. \n",
    "    \n",
    "    plot()\n",
    "    for j in range(repeat): discriminator.train_on_batch(x=disc_X, y=disc_y)\n",
    "    for j in range(repeat): gan.train_on_batch(x=noise, y=np.zeros(n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[[1]](http://papers.nips.cc/paper/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.pdf) Nowozin, S., Cseke, B. and Tomioka, R., 2016. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems (pp. 271-279).\n",
    "\n",
    "[[2]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5605355) Nguyen, X., Wainwright, M.J. and Jordan, M.I., 2010. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11), pp.5847-5861.\n",
    "\n",
    "[[3]](https://papers.nips.cc/paper/5423-generative-adversarial-nets) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
